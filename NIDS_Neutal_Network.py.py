# -*- coding: utf-8 -*-
"""Masters-NN-NIDS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p4JCj1WQPOY1TjIXSeGmRA1_b551Wmdp
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive #import google colab drive package to connect to my google drive where the my dataset will be located
drive.mount("/content/drive")

#outlining the names for my independent variables columns in my dataset in a large string
cols="""duration,
protocol_type,
service,
flag,
src_bytes,
dst_bytes,
land,
wrong_fragment,
urgent,
hot,
num_failed_logins,
logged_in,
num_compromised,
root_shell,
su_attempted,
num_root,
num_file_creations,
num_shells,
num_access_files,
num_outbound_cmds,
is_host_login,
is_guest_login,
count,
srv_count,
serror_rate,
srv_serror_rate,
rerror_rate,
srv_rerror_rate,
same_srv_rate,
diff_srv_rate,
srv_diff_host_rate,
dst_host_count,
dst_host_srv_count,
dst_host_same_srv_rate,
dst_host_diff_srv_rate,
dst_host_same_src_port_rate,
dst_host_srv_diff_host_rate,
dst_host_serror_rate,
dst_host_srv_serror_rate,
dst_host_rerror_rate,
dst_host_srv_rerror_rate"""

columns=[] #Initialize an empty list called columns where the final cleaned-up column names will be stored.

#Create a list with the col names from our string mentioned above and remove any whitespaces using an iterative for loop
for c in cols.split(','):
    if(c.strip()):
       columns.append(c.strip())

columns.append('target') #add the column target to the end of the list
print(columns)
print(len(columns)) #length of new list columns

#attacks_types is a dictionary, where each key represents a specific type of network attack,
#and the corresponding value represents the category or classification of that attack.
attacks_types = {
  'normal': 'normal', #format is 'attack type': 'attack_category'
'back': 'dos',
'buffer_overflow': 'u2r',
'ftp_write': 'r2l',
'guess_passwd': 'r2l',
'imap': 'r2l',
'ipsweep': 'probe',
'land': 'dos',
'loadmodule': 'u2r',
'multihop': 'r2l',
'neptune': 'dos',
'nmap': 'probe',
'perl': 'u2r',
'phf': 'r2l',
'pod': 'dos',
'portsweep': 'probe',
'rootkit': 'u2r',
'satan': 'probe',
'smurf': 'dos',
'spy': 'r2l',
'teardrop': 'dos',
'warezclient': 'r2l',
'warezmaster': 'r2l',
}
print(len(attacks_types))

path = "/content/drive/MyDrive/KDD_CUP/kddcup.data_10_percent.gz" #outlining path to dadatset on google drive
df = pd.read_csv(path,names=columns) #using pandas to read our dataset and outlining the names as our new created columns list

#Assign a new column to the DataFrame called 'Attack Type' which will contain the categories to each attack type from the target column
df['Attack Type'] = df.target.apply(lambda r:attacks_types[r[:-1]])

df.head()

df["Attack Type"].value_counts() #print the frequency (count) of each attack type in the 'Attack Type' column

#Visualization
def bar_g(feature): #define a function names bar_g (bar grapgh) that takes one parameter
  df[feature].value_counts().plot(kind="bar") #plot the feature frequncy counts in a bar chart

bar_g("protocol_type")

plt.figure(figsize=(10,3)) #using matplotlib to set fig size, as this features as a high unique count - without this it would be small and unreadable
bar_g("service")

plt.figure(figsize=(10,3))  #using matplotlib to set fig size, as this features as a high unique count - without this it would be small and unreadable
bar_g("flag")

plt.figure(figsize=(10,3))
bar_g("logged_in")

bar_g("target")

bar_g("target")

df.columns #printing the columns in our dataframe

#drop columns with NaN - as out dataset is so big we can afford to just remove NaN values
df = df.dropna("columns")

#Kepp columns where there are more than 1 unique values
df = df[[col for col in df if df[col].nunique() > 1 ]]

#getting the dadatsets correlation between varibales - any extremely high independent variables correlated with each other or with Y class 'attack types' eill need to assessed and removed
#If correlation is low and would not add any value to the Y (target) varibale - we can remove
corr = df.corr

plt.figure(figsize=(20,18)) #setting the figure size with matplotlib
sns.heatmap(df.corr(), annot=True, fmt=".2f") #using seaborn to plot a heatmap
plt.show()

#This variable is highly correlated with num_compromised and should be ignored for analysis.
#(Correlation = 0.9938277978738366)
df.drop("num_root",axis = 1, inplace = True)

#This variable is highly correlated with serror_rate and should be ignored for analysis.
#(Correlation = 0.9983615072725952
df.drop("srv_serror_rate", axis = 1, inplace = True)

#This variable is highly correlated with rerror_rate and should be ignored for analysis.
#(Correlation = 0.9947309539817937)
df.drop('srv_rerror_rate',axis = 1, inplace=True)

#This variable is highly correlated with srv_serror_rate and should be ignored for analysis.
#(Correlation = 0.9993041091850098)
df.drop('dst_host_srv_serror_rate',axis = 1, inplace=True)

#This variable is highly correlated with rerror_rate and should be ignored for analysis.
#(Correlation = 0.9869947924956001)
df.drop('dst_host_serror_rate',axis = 1, inplace=True)

#This variable is highly correlated with srv_rerror_rate and should be ignored for analysis.
#(Correlation = 0.9821663427308375)
df.drop('dst_host_rerror_rate',axis = 1, inplace=True)

#This variable is highly correlated with rerror_rate and should be ignored for analysis.
#(Correlation = 0.9851995540751249)
df.drop('dst_host_srv_rerror_rate',axis = 1, inplace=True)

#This variable is highly correlated with dst_host_srv_count and should be ignored for analysis.
#(Correlation = 0.9736854572953938)
df.drop('dst_host_same_srv_rate',axis = 1, inplace=True)

df.head()

#protocol_type the features - one hot encoding - mapping the network type to a numerial value
pmap = {"icmp":0, "tcp":1, "udp": 2}

df["protocol_type"] = df["protocol_type"].map(pmap)

df["protocol_type"]

#flag feature mapping - one hot encoding - mapping the flag data to a numerial value
fmap = {'SF':0,'S0':1,'REJ':2,'RSTR':3,'RSTO':4,'SH':5 ,'S1':6 ,'S2':7,'RSTOS0':8,'S3':9 ,'OTH':10}
df['flag'] = df['flag'].map(fmap)

df["flag"]

#attack type feature mapping - mapping our attack types to a numerical value - one hot encoding
amap = {"dos":0, "normal":1, "probe":2, "r2l":3, "u2r": 4}
df["Attack Type"] = df["Attack Type"].map(amap)

df["Attack Type"]

df.drop("service", axis =1, inplace=True) #dropping service column, outlined by axis=1 and doing it now in inplace

"""Building ad training a Neural Network"""

from sklearn.model_selection import train_test_split #using the train test split package to split the datatset - for train and test daatasets
from sklearn.preprocessing import MinMaxScaler #importing MinMaxScaler to scale our dataset between 0 and 1
from sklearn.metrics import accuracy_score #after the model has run we use the accuracy metrix to see how well the model performed

import tensorflow as tf
from keras.models import Sequential, Model #used to build our Neural Network layers
from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, Input, Concatenate, Add #Neural Networks layers used in our model

df = df.drop(["target",], axis=1) #drooping the target column as we have 'attack types' in the dataset
print(df.shape) #getting the shape of the dataset before we start to split our data

df.head() #print the first 5 rows of our dataset

print(df.shape) #print the shape of the dataset
#Target variable and train set

#assigning our X - independent variables
#assigning our Y - Dependent variable (target)
Y = df[["Attack Type"]] #Assigning 'Attack Type' column to the
X = df.drop(["Attack Type",], axis=1) #using the drop method to drop the 'Attack Type' column

sc = MinMaxScaler() # creating an instance of the MinMaxScaler() class
X = sc.fit_transform(X) # MinMaxScaler on our X data

#Split test and train data
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.3, random_state=42) #splitting our data 70% train and Test 30%
print(X_train.shape, X_test.shape)
print(Y_train.shape, Y_test.shape)



df.to_csv("ids.csv", index=False) #using pandas dataframe method - to write the contents to a csv file

pd.read_csv("ids.csv")

"""Shallow Neural Network"""

#Here we are building out Neural Network model using the Sequential class
#This is a shallow model, 1 inout layer, a Dropout layer and an output layer
#input_dim indicates 30 independent variables being fed into the neural network
shallow_model = Sequential([
    Dense(1024, input_dim=32, activation='relu'), #'relu' is an activation function (0,max) - non-linearailty - dense layer is fully connected layer
    Dropout(0.01), #Droput is regularization technique used to prevent overfitting, during each training step, 1% of the neurons will ne randomly set to zero
    Dense(5, activation='softmax') #softmax AF, used for multi-class classification - 5 attack types - raw scores to probabilites
])

#Compiling the model
shallow_model.compile(loss="sparse_categorical_crossentropy", #loss function - used for multi-class classification
                      optimizer="adam", #optimization algorithm to adjust the weights of the model during training
                      metrics=["accuracy"] #calculate the percentages of correct predictions
              )

tf.keras.utils.plot_model(shallow_model, to_file="shallow_model.png", show_shapes=True) #printing a visual of our model shape

#fitting the model to our training data
#Y_train.ravel() - this method flattens target array - conerting from a 2D to 1D array (a flat vector)
#Data will be through the model 10 times
#Batch 32 - after 32 samples are passed through the model, the weights will be adjusted
history = shallow_model.fit(X_train, Y_train.values.ravel(), epochs=10, batch_size=32)

#This is a deeper model - which has more a lot more hidden layers
deep_model = Sequential([
    Dense(1024, input_dim=30, activation='relu'),
    Dropout(0.01),
    Dense(768, activation='relu'),
    Dropout(0.01),
    Dense(512, activation='relu'),
    Dropout(0.01),
    Dense(256, activation='relu'),
    Dropout(0.01),
    Dense(128, activation='relu'),
    Dropout(0.01),
    Dense(5, activation='softmax')
])

deep_model.compile(loss="sparse_categorical_crossentropy",
                   optimizer="adam",
                   metrics=["accuracy"])

tf.keras.utils.plot_model(deep_model, to_file="deep_model.png", show_shapes=True)

#fitting the model to our training data
#Y_train.ravel() - this method flattens target array - conerting from a 2D to 1D array (a flat vector)
#Data will be through the model 10 times
#Batch 32 - after 32 samples are passed through the model, the weights will be adjusted
deep_model.fit(X_train, Y_train.values.ravel(), epochs=10, batch_size=32)

"""Prediction of the Neural Network"""

predictions = shallow_model.predict(X_train) #using our trained shallow model, use the keras method perdeict to make predictions on our train data
predictions[26354] #predictions is a variable that stores the output of the model's predict() method

#In the case of a classification model with a softmax activation in the output layer (as shown in your earlier code), the predictions will be probabilities for each class for each input.
#These probabilities will sum up to 10 for each input.

#np.argmax() is a function from the NumPy library. It returns the index of the maximum value in the given array or list.
np.argmax(predictions[26354])

#Running our Neural Network model on our test data
shallow_test = shallow_model.predict(X_test)
Shallow_preds_train = shallow_model.predict(X_train)

#Printing the accuracy of our model on our test data
print("Shallow Neural Network")
print("Training Accuracy: " , accuracy_score(Y_train, np.argmax(Shallow_preds_train, axis=1)))
print("Testng Accuracy: ", accuracy_score(Y_test, np.argmax(shallow_test, axis=1)))